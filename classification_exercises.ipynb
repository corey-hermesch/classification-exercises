{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7814400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from env import host, user, password\n",
    "import os\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44683aa",
   "metadata": {},
   "source": [
    "In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data.\n",
    "\n",
    "    print the first 3 rows\n",
    "    print the number of rows and columns (shape)\n",
    "    print the column names\n",
    "    print the data type of each column\n",
    "    print the summary statistics for each of the numeric variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a couple ways:\n",
    "py_iris_df = data('iris')\n",
    "iris_df = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812844a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ed39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a80e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f8ac1",
   "metadata": {},
   "source": [
    "Read the data from this google sheet into a dataframe, df_google.\n",
    "\n",
    "'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit?usp=sharing'\n",
    "\n",
    "(Once you click on that, you get this one which is what you need:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357\n",
    ")\n",
    "\n",
    "\n",
    "    print the first 3 rows\n",
    "    print the number of rows and columns\n",
    "    print the column names\n",
    "    print the data type of each column\n",
    "    print the summary statistics for each of the numeric variables\n",
    "    print the unique values for each of your categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_export_url(g_sheet_url):\n",
    "    '''\n",
    "    This function will\n",
    "    - take in a string that is a url of a google sheet\n",
    "      of the form \"https://docs.google.com ... /edit#gid=12345...\"\n",
    "    - return a string that can be used with pd.read_csv\n",
    "    '''\n",
    "    csv_url = g_sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "    return csv_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebfdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_url = get_csv_export_url(sheet_url)\n",
    "csv_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google = pd.read_csv(csv_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9027d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c03f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_google.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ec166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.Sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ca76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.Ticket.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2153e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe34f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51713d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_google.columns.to_list():\n",
    "    if df_google[col].dtype == 'object':\n",
    "        print (f\"{col} has {df_google[col].nunique()} unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way. Thanks Cely!\n",
    "df_google.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e8d30",
   "metadata": {},
   "source": [
    "Download the previous exercise's file into an excel (File → Download → Microsoft Excel). Read the downloaded file into a dataframe named df_excel.\n",
    "\n",
    "    assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "    print the number of rows of your original dataframe\n",
    "    print the first 5 column names\n",
    "    print the column names that have a data type of object\n",
    "    compute the range for each of the numeric variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER variables for passing to excel, sheet_name is a big one:\n",
    "# pd.read_excel('LEA.xlsx', sheet_name=\"LEA Allocations\", header= 1)\n",
    "\n",
    "df_excel = pd.read_excel('train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffca741",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample = df_excel.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7425e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.columns[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.columns[:5].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f842f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.select_dtypes(exclude='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbeb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9762680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.select_dtypes(include=['int64','float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da713305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Must be an easier way to do this than 7 lines of code, but I'm going to skip it for now\n",
    "## I came back and this is easier. Still needs some formatting, but I'm out of time.\n",
    "for col in df_excel.select_dtypes(include=['int64','float64']).columns:\n",
    "    print(f\"{col}: max = {df_excel[col].max()}; min = {df_excel[col].min()}; range = {df_excel[col].max() - df_excel[col].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, Age and Fare are about the only ones where it makes sense to look at a range, so we could exclude the rest\n",
    "titanic_stats = df_excel[['Age', 'Fare']].describe().T\n",
    "titanic_stats['range'] = titanic_stats['max'] - titanic_stats['min']\n",
    "titanic_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bfcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PassengerId range = {df_excel.PassengerId.max() - df_excel.PassengerId.min()}\")\n",
    "print(f\"Survived range = {df_excel.Survived.max() - df_excel.Survived.min()}\")\n",
    "print(f\"Pclass range = {df_excel.Pclass.max() - df_excel.Pclass.min()}\")\n",
    "print(f\"Age range = {df_excel.Age.max() - df_excel.Age.min()}\")\n",
    "print(f\"SibSp range = {df_excel.SibSp.max() - df_excel.SibSp.min()}\")\n",
    "print(f\"Parch range = {df_excel.Parch.max() - df_excel.Parch.min()}\")\n",
    "print(f\"Fare range = {df_excel.Fare.max() - df_excel.Fare.min()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b95245",
   "metadata": {},
   "source": [
    "Make a new python module, acquire.py to hold the following data aquisition functions:\n",
    "\n",
    "    Make a function named get_titanic_data that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the Codeup Data Science Database.\n",
    "\n",
    "    Make a function named get_iris_data that returns the data from the iris_db on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the species_ids. Obtain your data from the Codeup Data Science Database.\n",
    "\n",
    "    Make a function named get_telco_data that returns the data from the telco_churn database in SQL. In your SQL, be sure to join contract_types, internet_service_types, payment_types tables with the customers table, so that the resulting dataframe contains all the contract, payment, and internet service options. Obtain your data from the Codeup Data Science Database.\n",
    "\n",
    "    Once you've got your get_titanic_data, get_iris_data, and get_telco_data functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of telco.csv, titanic.csv, or iris.csv. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af305bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sns_data(sns_source='iris'):\n",
    "    '''\n",
    "    This function will \n",
    "    - take in a string with the name of a seaborn dataset (default value is 'iris')\n",
    "    - return a dataframe after reading the sns dataset\n",
    "    '''\n",
    "    iris_df = sns.load_dataset(sns_source)\n",
    "    return iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68448670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_url(db_name, user=user, host=host, password=password):\n",
    "    '''\n",
    "    get_db_url accepts a database name, username, hostname, password \n",
    "    and returns a url connection string formatted to work with codeup's \n",
    "    sql database.\n",
    "    Default values from env.py are provided for user, host, and password.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic function to get a sql pull into a dataframe\n",
    "def get_mysql_data(sql_query, database):\n",
    "    \"\"\"\n",
    "    This function will:\n",
    "    - take in a sql query and a database (both strings)\n",
    "    - create a connection url to mySQL database\n",
    "    - return a df of the given query, connection_url combo\n",
    "    \"\"\"\n",
    "    url = get_db_url(database)\n",
    "    return pd.read_sql(sql_query, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titanic_data(sql_query=\"SELECT * FROM passengers\"\n",
    "                     , filename=\"titanic.csv\"):\n",
    "    \"\"\"\n",
    "    This function will:\n",
    "    -input 2 strings: sql_query, filename \n",
    "        default query \"SELECT * FROM passengers\"\n",
    "        default filename \"titanic.csv\"\n",
    "    - check the current working directory for filename (csv) existence\n",
    "      - return df from that filename if it exists\n",
    "    - If csv doesn't exist:\n",
    "      - create a df of the sql_query\n",
    "      - write df to csv\n",
    "      - return that df\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        print(\"csv file found and read\")\n",
    "        return df\n",
    "    else:\n",
    "        url = get_db_url('titanic_db')\n",
    "        df = pd.read_sql(sql_query, url)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print (\"csv file not found, data read from sql query, csv created\")\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_titanic_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = get_titanic_data()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_data(sql_query=\"SELECT * FROM species JOIN measurements USING (species_id)\"\n",
    "                 , filename=\"iris.csv\"):\n",
    "    \"\"\"\n",
    "    This function will:\n",
    "    -input 2 strings: sql_query, filename \n",
    "        default query \"SELECT * FROM passengers\"\n",
    "        default filename \"iris.csv\"\n",
    "    - check the current directory for filename (csv) existence\n",
    "      - return df from that filename if it exists\n",
    "    - If csv doesn't exist:\n",
    "      - create a df of the sql_query\n",
    "      - write df to csv\n",
    "      - return that df\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        print (\"csv file found and read\")\n",
    "        return df\n",
    "    else:\n",
    "        url = get_db_url('iris_db')\n",
    "        df = pd.read_sql(sql_query, url)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print (\"csv file not found, data read from sql query, csv created\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00529e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_iris_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_iris_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_telco_data(sql_query= \"\"\"\n",
    "                        SELECT  customer_id, gender, senior_citizen\n",
    "                            , partner, dependents, tenure, phone_service\n",
    "                            , multiple_lines, customers.internet_service_type_id\n",
    "                            , internet_service_types.internet_service_type\n",
    "                            , online_security, online_backup\n",
    "                            , device_protection, tech_support\n",
    "                            , streaming_tv, streaming_movies\n",
    "                            , customers.contract_type_id\n",
    "                            , contract_types.contract_type\n",
    "                            , paperless_billing, customers.payment_type_id\n",
    "                            , payment_types.payment_type\n",
    "                            , monthly_charges, total_charges\n",
    "                            , churn\n",
    "                        FROM customers\n",
    "                        JOIN contract_types USING (contract_type_id)\n",
    "                        JOIN internet_service_types USING (internet_service_type_id)\n",
    "                        JOIN payment_types USING (payment_type_id)\n",
    "                    \"\"\"\n",
    "                    , filename=\"telco.csv\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will:\n",
    "    -input 2 strings: sql_query, filename \n",
    "        default query \"SELECT * FROM passengers\"\n",
    "        default filename \"telco.csv\"\n",
    "    - check the current directory for filename (csv) existence\n",
    "      - return df from that filename if it exists\n",
    "    - If csv doesn't exist:\n",
    "      - create a df of the sql_query\n",
    "      - write df to csv\n",
    "      - return that df\n",
    "    \"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        print (\"csv file found and read\")\n",
    "        return df\n",
    "    else:\n",
    "        url = get_db_url('telco_churn')\n",
    "        df = pd.read_sql(sql_query, url)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print (\"csv file not found, data read from sql query, csv created\")\n",
    "        return df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f535c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b08359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_telco_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84846da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_telco_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire as acq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "newer_df = acq.get_iris_data()\n",
    "newer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_telco_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f905c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_titanic_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18d39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5684be32",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of this exercise should be the specified functions in a python script named prepare.py. Do these in your classification_exercises.ipynb first, then transfer to the prepare.py file.\n",
    "\n",
    "This work should all be saved in your local classification-exercises repo. Then add, commit, and push your changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfde326",
   "metadata": {},
   "source": [
    "### Using the Iris Data:\n",
    "\n",
    "    Use the function defined in acquire.py to load the iris data.\n",
    "\n",
    "    Drop the species_id and measurement_id columns.\n",
    "\n",
    "    Rename the species_name column to just species.\n",
    "\n",
    "    Create dummy variables of the species name and concatenate onto the iris dataframe. (This is for practice, we don't always have to encode the target, but if we used species as a feature, we would need to encode it).\n",
    "\n",
    "    Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire as acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca542f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff658639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['species_id', 'measurement_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'species_name': 'species'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2964cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f98c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df[['species']], drop_first=True)\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, dummy_df], axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae878b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_iris(df):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - take in the iris_df\n",
    "    - clean it up (remove useless columns, rename a column, and tack on some dummies columns for 'species')\n",
    "    - returns cleaned up dataframe\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=['species_id', 'measurement_id'])\n",
    "    df = df.rename(columns={'species_name': 'species'})\n",
    "    dummy_df = pd.get_dummies(df[['species']], drop_first=True)\n",
    "    new_df = pd.concat([df, dummy_df], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_iris(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0e6c0",
   "metadata": {},
   "source": [
    "### Using the Titanic dataset\n",
    "\n",
    "    Use the function defined in acquire.py to load the Titanic data.\n",
    "\n",
    "    Drop any unnecessary, unhelpful, or duplicated columns.\n",
    "\n",
    "    Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "    Create a function named prep_titanic that accepts the raw titanic data, and returns the data with the transformations above applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e96b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing[missing>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22856189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67963394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['passenger_id', 'class', 'embark_town', 'deck', 'age'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251226d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_df = pd.get_dummies(df[['pclass', 'sex','embarked']], drop_first=True)\n",
    "dummies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7adf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_df.pclass.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ceafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_df[['sex_male','embarked_Q','embarked_S']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, dummies_df], axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().sum() # two null values in embarked. We will practice with imputer during the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe874af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_titanic(df):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - take in the titanic dataframe\n",
    "    - clean it up (remove useless columns and tack on some dummies columns for 'pclass', 'sex', 'embarked')\n",
    "    - returns cleaned up dataframe\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=['passenger_id', 'class', 'embark_town', 'deck', 'age'])\n",
    "    dummies_df = pd.get_dummies(df[['pclass', 'sex','embarked']], drop_first=True)\n",
    "    new_df = pd.concat([df, dummies_df], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_titanic(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5560c",
   "metadata": {},
   "source": [
    "### Using the Telco dataset\n",
    "\n",
    "    Use the function defined in acquire.py to load the Telco data.\n",
    "\n",
    "    Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns but keeping the corresponding string values, for example.\n",
    "\n",
    "    Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "    Create a function named prep_telco that accepts the raw telco data, and returns the data with the transformations above applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4640d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.partner.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['customer_id', 'internet_service_type_id', 'contract_type_id', 'payment_type_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'senior_citizen': 'is_senior'\n",
    "                       ,'phone_service': 'phone_svc'\n",
    "                       ,'multiple_lines': 'mult_lines'\n",
    "                       ,'internet_service_type': 'intrnt_svc_type'\n",
    "                       ,'streaming_tv': 'strmg_tv'\n",
    "                       ,'streaming_movies': 'strmg_movies'\n",
    "                       ,'paperless_billing': 'paperless_bill'})\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f7183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mask = df.total_charges == ' '\n",
    "# mask.sum()\n",
    "# df.loc[mask, 'total_charges'] = '0'\n",
    "# df.total_charges = df.total_charges.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way\n",
    "df.total_charges = df.total_charges.str.replace(' ', '0').astype(float)\n",
    "df.total_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_charges[df.total_charges == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode binary categorical variables into numeric values\n",
    "df['gender_encoded'] = df.gender.map({'Male': 0, 'Female': 1})\n",
    "df['partner_encoded'] = df.partner.map({'No': 0, 'Yes': 1})\n",
    "df['dependendents_encoded'] = df.dependents.map({'No': 0, 'Yes': 1})\n",
    "df['phone_svc_encoded'] = df.phone_svc.map({'No': 0, 'Yes': 1})\n",
    "df['paperless_bill_encoded'] = df.paperless_bill.map({'No': 0, 'Yes': 1})\n",
    "df['churn_encoded'] = df.churn.map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy_df = pd.get_dummies(df[['']])\n",
    "dummy_df = pd.get_dummies(df[['mult_lines', 'intrnt_svc_type', 'online_security', 'online_backup'\n",
    "                               ,'device_protection', 'tech_support', 'strmg_tv', 'strmg_movies'\n",
    "                               ,'contract_type', 'payment_type']], drop_first=True)\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25307610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5537b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_telco(df):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - take in the telco_churn dataframe\n",
    "    - clean it up (remove useless columns, rename some columns, and\n",
    "      add encoded columns for categorical variables (columns) \n",
    "    - returns cleaned up (prepared) dataframe\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=['customer_id', 'internet_service_type_id', 'contract_type_id', 'payment_type_id'])\n",
    "    df = df.rename(columns={'senior_citizen': 'is_senior'\n",
    "                       ,'phone_service': 'phone_svc'\n",
    "                       ,'multiple_lines': 'mult_lines'\n",
    "                       ,'internet_service_type': 'intrnt_svc_type'\n",
    "                       ,'streaming_tv': 'strmg_tv'\n",
    "                       ,'streaming_movies': 'strmg_movies'\n",
    "                       ,'paperless_billing': 'paperless_bill'})\n",
    "    df.total_charges = df.total_charges.str.replace(' ', '0').astype(float)\n",
    "    df['gender_encoded'] = df.gender.map({'Male': 0, 'Female': 1})\n",
    "    df['partner_encoded'] = df.partner.map({'No': 0, 'Yes': 1})\n",
    "    df['dependendents_encoded'] = df.dependents.map({'No': 0, 'Yes': 1})\n",
    "    df['phone_svc_encoded'] = df.phone_svc.map({'No': 0, 'Yes': 1})\n",
    "    df['paperless_bill_encoded'] = df.paperless_bill.map({'No': 0, 'Yes': 1})\n",
    "    df['churn_encoded'] = df.churn.map({'No': 0, 'Yes': 1})\n",
    "    dummy_df = pd.get_dummies(df[['mult_lines', 'intrnt_svc_type', 'online_security', 'online_backup'\n",
    "                               ,'device_protection', 'tech_support', 'strmg_tv', 'strmg_movies'\n",
    "                               ,'contract_type', 'payment_type']], drop_first=True)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_telco_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4964b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = prep.prep_telco(df)\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)\n",
    "    print(df[col].value_counts())\n",
    "    print(df[col].value_counts(normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddffbd8",
   "metadata": {},
   "source": [
    "### Split your data\n",
    "\n",
    "    Write a function to split your data into train, test and validate datasets. Add this function to prepare.py.\n",
    "\n",
    "    Run the function in your notebook on the Iris dataset, returning 3 datasets, train_iris, validate_iris and test_iris.\n",
    "\n",
    "    Run the function on the Titanic dataset, returning 3 datasets, train_titanic, validate_titanic and test_titanic.\n",
    "\n",
    "    Run the function on the Telco dataset, returning 3 datasets, train_telco, validate_telco and test_telco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_function(df, target_var):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - take in a dataframe (df) and a string (target_var)\n",
    "    - split the dataframe into 3 data frames: train (60%), validate (20%), test (20%)\n",
    "    -   while stratifying on the target_var\n",
    "    - And finally return the three dataframes in order: train, validate, test\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(df, random_state=42, test_size=.2, stratify=df[target_var])\n",
    "    \n",
    "    train, validate = train_test_split(train, random_state=42, test_size=.25, stratify=train[target_var])\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_telco_df, validate_telco_df, test_telco_df = split_function(df, 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08af18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prepared df: {df.shape}')\n",
    "print()\n",
    "print(f'Train: {train_telco_df.shape}')\n",
    "print(f'Validate: {validate_telco_df.shape}')\n",
    "print(f'Test: {test_telco_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26cf088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire as acq\n",
    "import prepare as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2e96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file found and read\n",
      "Prepared df: (150, 7)\n",
      "\n",
      "Train: (90, 7)\n",
      "Validate: (30, 7)\n",
      "Test: (30, 7)\n"
     ]
    }
   ],
   "source": [
    "df = acq.get_iris_data()\n",
    "df = prep.prep_iris(df)\n",
    "train_iris, validate_iris, test_iris = prep.split_function(df, 'species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d89912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file found and read\n",
      "Prepared df: (891, 12)\n",
      "\n",
      "Train: (534, 12)\n",
      "Validate: (178, 12)\n",
      "Test: (179, 12)\n"
     ]
    }
   ],
   "source": [
    "df = acq.get_titanic_data()\n",
    "df = prep.prep_titanic(df)\n",
    "train_titanic, validate_titanic, test_titanic = prep.split_function(df, 'survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47329ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file found and read\n",
      "Prepared df: (7043, 47)\n",
      "\n",
      "Train: (4225, 47)\n",
      "Validate: (1409, 47)\n",
      "Test: (1409, 47)\n"
     ]
    }
   ],
   "source": [
    "df = acq.get_telco_data()\n",
    "df = prep.prep_telco(df)\n",
    "train_telco, validate_telco, test_telco = prep.split_function(df, 'churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9db32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
